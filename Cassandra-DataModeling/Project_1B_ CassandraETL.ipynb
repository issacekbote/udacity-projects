{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from cassandra.query import dict_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list of filepaths to process original event csv data files\n",
    "- Get event data current folder and sub folder\n",
    "- Create a for loop to create list of files and filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processs the files to create the 'event_datafile_new.csv' data file csv that will be used for Apache Casssandra tables\n",
    "- Read csv files and extract each data row one by one and append it to list\n",
    "- Write data from the list to a new event data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_rows_list = [] \n",
    "    \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. ETL Pipeline to import data into Apache Cassandra tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Cluster session to make connection to Cassandra instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1']) \n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace 'udacity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the session to 'udacity' Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "    session.row_factory = dict_factory\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table 'songplayed_session' to answer below query. The query gets artist, song title and song length for a particular Session ID and Item in Session\n",
    "- select artist, song, length from songplayed_session WHERE sessionid=338 AND itemInSession = 4\n",
    "\n",
    "##### Primary Key:\n",
    "A compund primary key is used to uniquely identify the rows with 'sessionId' as partition key and 'itemInSession' as clustering key \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS songplayed_session \"\n",
    "query = query + \"(sessionId int, itemInSession int, artist text, song text, length double, PRIMARY KEY (sessionId, itemInSession))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code extracts data from 'event_datafile_new' csv file and inserts into the songplayed_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO songplayed_session (sessionid, itemInSession, artist, song, length)\"\n",
    "        query = query + \" VALUES (%s, %s, %s, %s, %s)\"\n",
    "        session.execute(query, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a SELECT query to verify the data and uses data frame to display the query output by making use of dictionary result row format and then converting it to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      artist    length                             song\n",
      "0  Faithless  495.3073  Music Matters (Mark Knight Dub)\n"
     ]
    }
   ],
   "source": [
    "songplayed_session_query = \"select artist, song, length from songplayed_session WHERE sessionid=338 AND itemInSession = 4\"\n",
    "try:\n",
    "    rows = session.execute(songplayed_session_query)\n",
    "    df = pd.DataFrame.from_dict(rows)\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table 'songplaylist_session' to answer below query. The query gets artist name, song title ordered by itemInSession and user's first and last name for a particular userid and sessionid.\n",
    "- select artist, song, firstname, lastname from songplaylist_session WHERE userid=10 AND sessionid=182\n",
    "\n",
    "##### Primary Key:\n",
    "A compund primary key is used to uniquely identify the rows with 'userid' and 'sessionId' as composite partition key and 'itemInSession' as clustering key in descending order. 'itemInSession' is used in descending order as descending queries are faster due to the nature of the storage engine and ascending order is more efficient to store.\n",
    "A composite partition key is used to break data into smaller chunks/logical sets to facilitate data retrieval and avoid hotspotting in writting data to one node repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS songplaylist_session \"\n",
    "query = query + \"(userid int, sessionId int, itemInSession int, artist text, song text, firstname varchar, lastname varchar, PRIMARY KEY ((userid, sessionid), itemInSession)) \\\n",
    "                    WITH CLUSTERING ORDER BY (itemInSession DESC)\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code extracts data from 'event_datafile_new' csv file and inserts into the songplaylist_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO songplaylist_session (userid, sessionid, itemInSession, artist, song, firstname, lastname)\"\n",
    "        query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "        session.execute(query, (int(line[10]), int(line[8]), int(line[3]), line[0], line[9], line[1], line[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a SELECT query to verify the data and uses data frame to display the query output by making use of dictionary result row format and then converting it to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              artist                                               song  \\\n",
      "0      Lonnie Gordon  Catch You Baby (Steve Pitron & Max Sanna Radio...   \n",
      "1  Sebastien Tellier                                          Kilometer   \n",
      "2       Three Drives                                        Greece 2000   \n",
      "3   Down To The Bone                                 Keep On Keepin' On   \n",
      "\n",
      "          user  \n",
      "0  Sylvie Cruz  \n",
      "1  Sylvie Cruz  \n",
      "2  Sylvie Cruz  \n",
      "3  Sylvie Cruz  \n"
     ]
    }
   ],
   "source": [
    "songplaylist_session_query = \"select artist, song, firstname, lastname from songplaylist_session WHERE userid=10 AND sessionid=182\"\n",
    "try:\n",
    "    rows = session.execute(songplaylist_session_query)\n",
    "    df = pd.DataFrame.from_dict(rows)\n",
    "    df['user'] = df['firstname'] + ' ' + df['lastname']\n",
    "    print(df[['artist', 'song', 'user']])\n",
    "except Exception as e:\n",
    "    print(e)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table 'userlist_ahaho' (ahaho stands for the song 'All Hands Against His Own' ) to answer below query. The query gets artist name, song title ordered by itemInSession and user's first and last name for a particular userid and sessionid.\n",
    "- select firstname, lastname from userlist_ahaho WHERE song = 'All Hands Against His Own'\n",
    "\n",
    "##### Primary Key:\n",
    "A compund primary key is used to uniquely identify the rows with 'song' as pertition key and 'userid' as clustering key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code extracts data from 'event_datafile_new' csv file and inserts into the userlist_ahaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a SELECT query to verify the data and uses data frame to display the query output by making use of dictionary result row format and then converting it to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               user\n",
      "0  Jacqueline Lynch\n",
      "1      Tegan Levine\n",
      "2      Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
